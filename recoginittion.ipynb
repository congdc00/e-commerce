{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# I. Thư viện"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1, fixed_image_standardization\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# II. Chuẩn bị"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gọi GPU"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "data_path = './data/annotations'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tải dữ liệu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "embeddings= torch.load(data_path + '/facelist.pth')\n",
    "names = np.load(data_path + '/username.npy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lấy camera"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_cap = cv2.VideoCapture(0)\n",
    "video_cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "video_cap.set(cv2.CAP_PROP_FRAME_HEIGHT,480)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Kích thước đầu vào lấy từ webcam"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "frame_size =  (640, 480)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# III Chuẩn hóa"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model MTCNN để tách khuôn mặt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(thresholds= [0.7, 0.7, 0.8] ,keep_all=True, device = device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Các hàm chuẩn hóa"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def standardization(img_tensor_tmp):\n",
    "    normalize = (img_tensor_tmp -127.5)/128\n",
    "    return normalize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def trans(img):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        fixed_image_standardization\n",
    "    ])\n",
    "    return transform(img)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# IV. Main"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Khởi tạo model InceptionResnetV1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\DINHCH~1\\AppData\\Local\\Temp/ipykernel_3276/2492686935.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m model = InceptionResnetV1(\n\u001B[0m\u001B[0;32m      2\u001B[0m     \u001B[0mclassify\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[0mpretrained\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"casia-webface\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m ).to(device)\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\facenet_pytorch\\models\\inception_resnet_v1.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, pretrained, classify, num_classes, dropout_prob, device)\u001B[0m\n\u001B[0;32m    252\u001B[0m             \u001B[0mBlock8\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mscale\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.20\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    253\u001B[0m         )\n\u001B[1;32m--> 254\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mblock8\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mBlock8\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnoReLU\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    255\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mavgpool_1a\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mAdaptiveAvgPool2d\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    256\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdropout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDropout\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdropout_prob\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\facenet_pytorch\\models\\inception_resnet_v1.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, scale, noReLU)\u001B[0m\n\u001B[0;32m    108\u001B[0m         self.branch1 = nn.Sequential(\n\u001B[0;32m    109\u001B[0m             \u001B[0mBasicConv2d\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1792\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m192\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkernel_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstride\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 110\u001B[1;33m             \u001B[0mBasicConv2d\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m192\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m192\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkernel_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstride\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpadding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    111\u001B[0m             \u001B[0mBasicConv2d\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m192\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m192\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkernel_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstride\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpadding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    112\u001B[0m         )\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\facenet_pytorch\\models\\inception_resnet_v1.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, in_planes, out_planes, kernel_size, stride, padding)\u001B[0m\n\u001B[0;32m     14\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0min_planes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mout_planes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkernel_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstride\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpadding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m         \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 16\u001B[1;33m         self.conv = nn.Conv2d(\n\u001B[0m\u001B[0;32m     17\u001B[0m             \u001B[0min_planes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mout_planes\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m             \u001B[0mkernel_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mkernel_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstride\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstride\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001B[0m\n\u001B[0;32m    431\u001B[0m         \u001B[0mpadding_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpadding\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0m_pair\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    432\u001B[0m         \u001B[0mdilation_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_pair\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdilation\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 433\u001B[1;33m         super(Conv2d, self).__init__(\n\u001B[0m\u001B[0;32m    434\u001B[0m             \u001B[0min_channels\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mout_channels\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkernel_size_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstride_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpadding_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdilation_\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    435\u001B[0m             False, _pair(0), groups, bias, padding_mode, **factory_kwargs)\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001B[0m\n\u001B[0;32m    136\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mregister_parameter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'bias'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    137\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 138\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreset_parameters\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    139\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    140\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mreset_parameters\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001B[0m in \u001B[0;36mreset_parameters\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    142\u001B[0m         \u001B[1;31m# uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    143\u001B[0m         \u001B[1;31m# For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 144\u001B[1;33m         \u001B[0minit\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkaiming_uniform_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0ma\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m5\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    145\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbias\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    146\u001B[0m             \u001B[0mfan_in\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minit\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_calculate_fan_in_and_fan_out\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\init.py\u001B[0m in \u001B[0;36mkaiming_uniform_\u001B[1;34m(tensor, a, mode, nonlinearity)\u001B[0m\n\u001B[0;32m    393\u001B[0m     \u001B[0mbound\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m3.0\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mstd\u001B[0m  \u001B[1;31m# Calculate uniform bounds from standard deviation\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    394\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 395\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mtensor\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0muniform_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mbound\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbound\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    396\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    397\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = InceptionResnetV1(\n",
    "    classify=False,\n",
    "    pretrained=\"casia-webface\"\n",
    ").to(device)\n",
    "\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Funtion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "extract_face: Trích xuất khuôn mặt từ các bounding box\n",
    "Thông số:\n",
    "- margin: Tương đương với margin khi capture_face ( để là 20)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_face(box, img, margin=20):\n",
    "    face_size = 160\n",
    "    img_size = frame_size\n",
    "    margin = [\n",
    "        margin * (box[2] - box[0]) / (face_size - margin),\n",
    "        margin * (box[3] - box[1]) / (face_size - margin),\n",
    "    ] #tạo margin bao quanh box cũ\n",
    "    box = [\n",
    "        int(max(box[0] - margin[0] / 2, 0)),\n",
    "        int(max(box[1] - margin[1] / 2, 0)),\n",
    "        int(min(box[2] + margin[0] / 2, img_size[0])),\n",
    "        int(min(box[3] + margin[1] / 2, img_size[1])),\n",
    "    ]\n",
    "    img = img[box[1]:box[3], box[0]:box[2]]\n",
    "    face = cv2.resize(img,(face_size, face_size), interpolation=cv2.INTER_AREA)\n",
    "    face = Image.fromarray(face)\n",
    "    return face"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Kết xuất embedding cho từng ảnh mặt - thứ đã được extract từ hàm extract_face() ở trên"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def inference(model, face, local_embeds, threshold = 3):\n",
    "    #local: [n,512] voi n la so nguoi trong faceslist\n",
    "    embeds = []\n",
    "    # print(trans(face).unsqueeze(0).shape)\n",
    "    embeds.append(model(trans(face).to(device).unsqueeze(0)))\n",
    "    detect_embeds = torch.cat(embeds) #[1,512]\n",
    "    # print(detect_embeds.shape)\n",
    "    #[1,512,1]                                      [1,512,n]\n",
    "    norm_diff = detect_embeds.unsqueeze(-1) - torch.transpose(local_embeds, 0, 1).unsqueeze(0)\n",
    "    # print(norm_diff)\n",
    "    norm_score = torch.sum(torch.pow(norm_diff, 2), dim=1) #(1,n), moi cot la tong khoang cach euclide so vs embed moi\n",
    "\n",
    "    min_dist, embed_idx = torch.min(norm_score, dim = 1)\n",
    "    print(min_dist*power, names[embed_idx])\n",
    "    # print(min_dist.shape)\n",
    "    if min_dist*power > threshold:\n",
    "        return -1, -1\n",
    "    else:\n",
    "        return embed_idx, min_dist.double()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "main"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "while video_cap.isOpened():\n",
    "    isSuccess, frame = video_cap.read()\n",
    "    if isSuccess:\n",
    "        boxes, _ = mtcnn.detect(frame)\n",
    "        if boxes is not None:\n",
    "            for box in boxes:\n",
    "                bbox = list(map(int,box.tolist()))\n",
    "                face = extract_face(bbox, frame)\n",
    "                idx, score = inference(model, face, embeddings)\n",
    "                if idx != -1:\n",
    "                    frame = cv2.rectangle(frame, (bbox[0],bbox[1]), (bbox[2],bbox[3]), (0,0,255), 6)\n",
    "                    score = torch.Tensor.cpu(score[0]).detach().numpy()*power\n",
    "                    frame = cv2.putText(frame, names[idx] + '_{:.2f}'.format(score), (bbox[0],bbox[1]), cv2.FONT_HERSHEY_DUPLEX, 2, (0,255,0), 2, cv2.LINE_8)\n",
    "                else:\n",
    "                    frame = cv2.rectangle(frame, (bbox[0],bbox[1]), (bbox[2],bbox[3]), (0,0,255), 6)\n",
    "                    frame = cv2.putText(frame,'Unknown', (bbox[0],bbox[1]), cv2.FONT_HERSHEY_DUPLEX, 2, (0,255,0), 2, cv2.LINE_8)\n",
    "\n",
    "        new_frame_time = time.time()\n",
    "        fps = 1/(new_frame_time-prev_frame_time)\n",
    "        prev_frame_time = new_frame_time\n",
    "        fps = str(int(fps))\n",
    "        cv2.putText(frame, fps, (7, 70), cv2.FONT_HERSHEY_DUPLEX, 3, (100, 255, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Face Recognition', frame)\n",
    "    if cv2.waitKey(1)&0xFF == 27:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "video_cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}